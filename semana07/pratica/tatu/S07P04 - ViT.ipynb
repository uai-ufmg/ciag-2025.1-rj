{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c582424c-5584-4e48-9725-d6896a606c1e",
   "metadata": {},
   "source": [
    "# ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea91b93-9e0e-4c86-b6d2-f72f678d2762",
   "metadata": {},
   "source": [
    "## Configuração\n",
    "\n",
    "Importando módulos necessários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3ed389-0da1-4af7-88f2-1837ae04812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nbproxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440cc8bd-cc03-4311-a173-5703d348ac6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas principais\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# Transformações e manipulação de dados\n",
    "import torchvision\n",
    "from torchvision import transforms, models, datasets\n",
    "\n",
    "# Utilitários de visualização e manipulação de dados\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Divisão de dados e métricas\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97747775-c84d-4a55-adc0-18ff0ec148c2",
   "metadata": {},
   "source": [
    "## Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd16069-916a-4278-89b5-b16cce3f81d7",
   "metadata": {},
   "source": [
    "Neste notebook, exploraremos o Vision Transformer (ViT), uma arquitetura baseada em Transformers que tem revolucionado a área de visão computacional. Tradicionalmente, redes neurais convolucionais (CNNs) eram consideradas o padrão para tarefas como classificação de imagens, detecção de objetos e segmentação. No entanto, o ViT demonstrou que Transformers, originalmente projetados para processamento de linguagem natural (NLP), também podem ser altamente eficazes para imagens.\n",
    "\n",
    "Diferentemente das CNNs, que extraem características por meio de convoluções locais, o ViT trabalha dividindo a imagem em patches (pequenos blocos) e processando-os como uma sequência, semelhante a tokens em NLP. Essa abordagem permite capturar dependências de longo alcance entre regiões da imagem desde as primeiras camadas da rede, por conta do uso do mecanismo de Attention, algo que as CNNs alcançam apenas em camadas mais profundas. Com treinamentos adequados e grandes volumes de dados, o ViT superou as CNNs em várias tarefas de visão computacional.\n",
    "\n",
    "O objetivo dos exercícios propostos neste notebook é revisitar os conceitos da aula de ViT e analisar como os mesmos problemas vistos no módulo de CNNs se comportam quando resolvidos com Vision Transformers (ViTs). Iniciaremos com um exercício introdutório, no qual retomaremos a tarefa de classificação de imagens no dataset CIFAR-10, já explorada em semanas anteriores, para que entendam e se familiarizem com implementação do modelo. Em seguida, no próximo notebook, avançaremos para um desafio mais complexo: a classificação litológica utilizando outro dataset já visto, o DRP-Benchmarks, especificamente as bases Berea e Grosmont. No módulo de CNNs, essa tarefa foi abordada por meio de uma U-Net para segmentação. Agora, exploraremos como o ViT pode ser aplicado a esse mesmo problema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e96907-1d8a-4c8a-a742-8b45c8bcd7df",
   "metadata": {},
   "source": [
    "## CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2f148d-ffba-4e15-be87-e89b46f4640d",
   "metadata": {},
   "source": [
    "Definindo argumentos para a implementação do modelo e fazendo o carregamento dos dados, augumentation e outras transformações. Feito de forma similar aos notebooks anteriores da semana de CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a556f2-1c41-4aba-ba3f-35d871f4a98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'epoch_num': 50,\n",
    "    'n_classes': 10,\n",
    "    'lr': 1e-4,\n",
    "    'weight_decay': 5e-4,\n",
    "    'momentum': 0.9,\n",
    "    'num_workers': 4,\n",
    "    'batch_size': 128,\n",
    "    'w_size': 32, # largura da imagem para redimensionar\n",
    "    'h_size': 32, # altura da imagem para redimensionar\n",
    "}\n",
    " \n",
    "if torch.cuda.is_available():\n",
    "    args['device'] = torch.device('cuda')\n",
    "else:\n",
    "    args['device'] = torch.device('cpu')\n",
    " \n",
    "print(args['device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65650193-1a58-4fa3-8e88-5a97d5b5992f",
   "metadata": {},
   "source": [
    "### Pré-processamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47044a2f-e382-4387-8051-5b707889a6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/pgeoprj2/ciag2024/dados'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # para centrar os dados e facilitar a convergecia\n",
    "])\n",
    "\n",
    "data_transform1 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normaliza os valores das imagens\n",
    "])\n",
    " \n",
    "data_transform2 = transforms.Compose([\n",
    "    transforms.Resize((100, 100)),\n",
    "    transforms.RandomCrop((75, 75)),\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5), # Faz um augumentation\n",
    "])\n",
    " \n",
    "data_transform3 = transforms.Compose([\n",
    "    transforms.Resize((args['h_size'], args['w_size'])), # Rearranja para o uso no modelo\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Aplico as tranformações\n",
    "trainset = datasets.CIFAR10(root, train=True, download=True, transform=data_transform1)\n",
    "testset = datasets.CIFAR10(root, train=False, download=True, transform=data_transform1)\n",
    " \n",
    "trainset = datasets.CIFAR10(root, train=True, download=True, transform=data_transform2)\n",
    "testset = datasets.CIFAR10(root, train=False, download=True, transform=data_transform2)\n",
    " \n",
    "trainset = datasets.CIFAR10(root, train=True, download=True, transform=data_transform3)\n",
    "testset = datasets.CIFAR10(root, train=False, download=True, transform=data_transform3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef36ccc5-58cc-4598-b59c-cb40c99a8fd3",
   "metadata": {},
   "source": [
    "### Definindo o Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d5d540-8d9a-44b4-81ed-9ac6ca0106bc",
   "metadata": {},
   "source": [
    "#### 1) Crie o ViTDataset\n",
    "> Carregue todos os dados `testset`, `trainset`, e divida a parte de validação nos dados de treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49599853-739b-42a2-87df-01182c390a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crio a Classe Dataset para organizar os dados em treino, teste e validação\n",
    "class ViTDataset(Dataset):\n",
    "    def __init__(self, trainset, testset, split, transform=None):\n",
    "\n",
    "        # 1) Defina self.split\n",
    "        # 2) Defina self.transform\n",
    "        # 3) Defina self.dataset de acordo com o split\n",
    "        # if self.split == 'test':\n",
    "        #     ...\n",
    "        # else:\n",
    "        #     Divida train como 80% e validation como 20%\n",
    "        #     if self.split == 'train':\n",
    "        #     elif self.split == 'validation':\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # Defina\n",
    "        return ...\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Defina e aplique a transformação se houver\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b849b3d0-a413-4a94-a5b2-3f49f2b19f33",
   "metadata": {},
   "source": [
    "### Datasets e Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db261a4-1574-4bc6-bb44-437573fde9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria os Datasets\n",
    "train_vit_dataset = ViTDataset(trainset, testset, \"train\", transform=transform) \n",
    "val_vit_dataset = ViTDataset(trainset, testset, \"validation\", transform=transform)\n",
    "test_vit_dataset = ViTDataset(trainset, testset, \"test\", transform=transform)\n",
    " \n",
    "# Criar os Dataloaders\n",
    "trainloader = DataLoader(train_vit_dataset, batch_size=args['batch_size'], num_workers=args['num_workers'], shuffle=True)\n",
    "valloader = DataLoader(val_vit_dataset, batch_size=args['batch_size'], num_workers=args['num_workers'], shuffle=False)\n",
    "testloader = DataLoader(test_vit_dataset, batch_size=args['batch_size'], num_workers=args['num_workers'], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c611a94f-b18d-4342-bcf6-c1183c7980df",
   "metadata": {},
   "source": [
    "### Alguns exemplos do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44416e04-66b5-49ae-9320-436208c3f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))  # Reorganiza o formato da imagem\n",
    "    plt.show()\n",
    "\n",
    "# Plotando exemplos do TRAINSET/TESTSET\n",
    "NUM_IMAGES = 4\n",
    "IMAGES = torch.stack([trainset[idx][0] for idx in range(NUM_IMAGES)], dim=0)\n",
    "LABELS = [trainset[idx][1] for idx in range(NUM_IMAGES)]\n",
    "img_grid = torchvision.utils.make_grid(IMAGES, nrow=4, normalize=True, pad_value=0.9)\n",
    "img_grid = img_grid.permute(1, 2, 0)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.title(\"Exemplos - CIFAR10 dataset\")\n",
    "plt.imshow(img_grid)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a09de02-e35f-47af-bee0-58dc34e9378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando um lote do dataloader - tamanhos [batch_size, channels, image_h, image_w] \n",
    "for images, labels in trainloader:\n",
    "    print(images.shape)  # Esperado: [batch_size, channels, image_size, image_size]\n",
    "    print(labels.shape)  # Esperado: [batch_size]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfe2da1-b79a-4335-8a36-1b5a257de213",
   "metadata": {},
   "source": [
    "Agora, a partir daqui temos os dados organizados em um Dataset que gera os Dataloaders que utilizaremos no nosso modelo, de treino, teste e validação. Observe que nosso `ViTDataset` retorna um tensor de formato `[128, 3, 32, 32]`, ou seja, retorna uma imagem inteira `32x32`, de três canais e batch de `128`. Precisamos decidir como cortar essa imagem em patches para passar para nosso ViT posteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52c3125-5922-46d6-a38c-3774b33bbeb8",
   "metadata": {},
   "source": [
    "## Como partir as imagens em patches?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49679fcf-32be-4a84-b3a4-142dbcbbf72f",
   "metadata": {},
   "source": [
    "Cortar uma imagem em patches para um Vision Transformer (ViT) é uma tarefa relativamente simples, mas requer atenção na escolha do tamanho dos patches. O ideal é selecionar um tamanho que seja divisível pelas dimensões da imagem, garantindo um corte preciso e sem sobreposição. Além disso, os patches não devem ser excessivamente pequenos, pois isso pode resultar na perda de informações relevantes. Encontrar um equilíbrio entre resolução e número de patches é essencial para que o modelo capture bem os padrões visuais. Abaixo há uma função de corte de referência, mas que serve mais de exemplo para a visualização dessas técnica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f578c462-296a-4654-9030-91088f54b097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_patches(image, patch_size=8): # Apenas para visulização\n",
    "    patches = []\n",
    "    for i in range(0, image.size(1), patch_size):\n",
    "        for j in range(0, image.size(2), patch_size):\n",
    "            patch = image[:, i:i+patch_size, j:j+patch_size]\n",
    "            patches.append(patch)\n",
    "    return torch.stack(patches) # Retorna um tensor [16, 3, 8, 8] -> [num_de_patches, canais, patch_h, patch_w]\n",
    " \n",
    "def imshow_patches(patches, grid_size=(4, 4)):\n",
    "    patches_grid = torchvision.utils.make_grid(patches, nrow=grid_size[1]) \n",
    "    imshow(patches_grid)\n",
    " \n",
    "class_names = trainset.classes # dicionário de número da label p/ nome da label\n",
    "patches = image_to_patches(IMAGES[1])\n",
    "label = LABELS[1]\n",
    "print(f\"Label da imagem: {label} ({class_names[label]})\")  # Imprimir o número da label e o nome da classe\n",
    "imshow_patches(patches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcf0d28-a097-45dd-86ed-48e2d847bd1e",
   "metadata": {},
   "source": [
    "## Transformers para classificação de imagens  \n",
    "\n",
    "Transformers são um tipo de arquitetura de rede neural baseada em mecanismos de atenção (principalmente self-attention) para processar sequências de dados de forma eficiente. Eles foram introduzidos no artigo [\"Attention Is All You Need\" (2017)](https://arxiv.org/abs/1706.03762) e revolucionaram o processamento de linguagem natural (PLN) e visão computacional. Diferente de redes recorrentes (RNNs), os Transformers processam tokens em paralelo, tornando o treinamento mais rápido e eficiente. Seu principal componente, o mecanismo de atenção, permite que cada elemento da sequência \"preste atenção\" em outros elementos, capturando relações de longo alcance. Mas por que não aplicar essa mesma ideia em imagens?  \n",
    "\n",
    "Foi exatamente isso que Alexey Dosovitskiy et al. propuseram no artigo [\"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\"](https://openreview.net/pdf?id=YicbFdNTTy). O **Vision Transformer (ViT)** trata imagens como sequências de pequenos **patches**. Por exemplo, uma imagem de 48×48 pixels pode ser dividida em 9 patches de 16×16 pixels. Cada patch é tratado como um **token**, projetado para um espaço de características. Ao adicionar codificações posicionais e um token de classificação, podemos aplicar um Transformer nessa sequência e treiná-lo para a tarefa de classificação de imagens.  \n",
    "\n",
    "Abaixo, uma animação ilustra essa ideia ([Phil Wang](https://github.com/lucidrains/vit-pytorch/blob/main/images/vit.gif)):  \n",
    "\n",
    "<!-- ![Vision Transformer gif](https://github.com/lucidrains/vit-pytorch/raw/main/images/vit.gif)  -->\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://github.com/lucidrains/vit-pytorch/raw/main/images/vit.gif\" alt=\"Vision Transformer gif\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "Aqui focaremos na parte do Encoder do Transformer, nele utiliza-se o mecanismo de atenção para extrair representações complexas de dados, como imagens ou textos. Esse mecanismo permite que o modelo foque nas partes mais relevantes da entrada, capturando as características essenciais para a tarefa. Após a extração das features, o Transformer pode ser combinado com diferentes tipos de cabeças (heads). No exemplo mostrado acima, a cabeça de classificação é representada por uma MLP (perceptron multicamadas), que atua como um classificador para a tarefa específica, mas pode até mesmo ser integrados com decoders, como já visto, para tarefas como tradução e geração de texto por exemplo.\n",
    "\n",
    "Sua flexibilidade permite que o Transformer seja facilmente adaptado para diversas tarefas, como classificação, segmentação e modelos multimodais que combinam dados textuais e visuais. Essa versatilidade torna o Transformer uma ferramentaamplamente utilizada no cenário atual, capaz de ser integrada com outras arquiteturas de redes neurais, resultando em modelos mais complexos, versáteis e poderosos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c64929d-1aac-43f9-bdb9-7c8325f8f891",
   "metadata": {},
   "source": [
    "## Implementação de um Visual Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716913a6-3ffe-467e-ab7c-bb654d12c7cb",
   "metadata": {},
   "source": [
    "### 2) Defina a função para cortar a imagem em patches\n",
    "> Implemente uma função de \"image to patch\". Não é recomendado tentar reutilizar a função de exemplo anterior, pois seguindo o modelo abaixo é mais intuitivo e simples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723418da-c412-4c9c-a862-9d2b0bb52e09",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*cnbRaydTbaTJK2-Q7DAJBA.png\" alt=\"Vision Transformer gif\" width=\"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfe5a13-10ad-4011-8c5c-4b10ffbd4080",
   "metadata": {},
   "source": [
    "Como fazer: Imagine que você possui uma imagem de tamanho `32x32` e deseja dividi-la em patches. Primeiramente teríamos `B, C, H, W = image.shape` como formato da imagem, e vamos dividí-la conforme patches de tamanho `patch_size_h` por `patch_size_w`, resultando em `[B, C, H // patch_size_h, patch_size_h, W // patch_size_w, patch_size_w]`, Isso cria uma \"grade\" de patches dentro da imagem, mas esse formato não é conveniente para alimentar um ViT, ois o modelo precisa que os patches sejam representados como vetores em uma sequência de entradas. Para isso reorganizamos o formato para `[B, H', W', C, patch_H, patch_W]`, com `H'` e `W'` como o número de patches na altura e largura, e depois unimos, `[B, H' * W', C * patch_H * patch_W]`, de forma que temos o `batch_size`, `num_patches`, o vetor que representa um patch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0b808c-60ac-47a8-a825-8cd55f6507f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_patch(x, patch_size):\n",
    "\n",
    "    # 0) Defina x.shape\n",
    "    # 1) .reshape para deividir em patches\n",
    "    # 2) .permute para reorganizar o vetor\n",
    "    # 3) .flatten para unir valores\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9cf7cf-68d8-4c25-a36c-abd5d38cfe80",
   "metadata": {},
   "source": [
    "### 3) Defina a Camada do Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2923556-ce9f-4c76-9dc3-5cf09ed6588b",
   "metadata": {},
   "source": [
    "Aqui será feita a atenção uitlizando o `MultiheadAttention` já visto com detalhes anteriormente. A ideia aqui é fazer a normalização pré-layer, isso é apenas uma decisão de implementação, mas que de acordo com [Ruibin Xionget al.](https://arxiv.org/abs/2002.04745) em 2020. A ideia é aplicar a Layer Normalization não entre os blocos residuais, mas sim como a primeira camada dentro desses blocos. Essa reorganização das camadas ajuda a melhorar o fluxo dos gradientes. Essa ideia é apresentada na imagem abaixo:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"../../images/Layer_Normalization.png\" alt=\"Vision Transformer gif\" width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf33db9-8cdf-48ad-974e-b67895f63c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1) Defina uma norm_layer1 que recebe (embed_dim)\n",
    "        # 2) Defina uma camada de atenção\n",
    "        # 3) Defina uma norm_layer2 que recebe (embed_dim)\n",
    "        # 4) Defina uma sequência de operações lineares e função de ativação (ReLU ou GELU) e dropout\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 1) Defina o \"forward\" da rede, lembre-se de usar uma variável para guardar o a saída da primeira camada (norm_layer) para usá-la no Attention\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7c4a97-2664-45ad-8530-c6baf4a5129c",
   "metadata": {},
   "source": [
    "### 4) Defina o ViT Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae24090-9f08-4fb5-8788-ebdcfa8d4e6f",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*qab5J7abx-XTNf9CPJ15dw.png\" alt=\"Vision Transformer gif\" width=\"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af13b452-fab3-425b-aa34-afd4068897d9",
   "metadata": {},
   "source": [
    "A implementação do **Vision Transformer (ViT)** pode ser resumida em:\n",
    "\n",
    "1. **Inicialização do Modelo (`__init__`)**:\n",
    "   - **`input_layer`**: Converte os patches das imagens em embeddings com dimensionalidade `embed_dim`.\n",
    "   - **`transformer`**: Composto por várias camadas de **TransformerLayer** que aplicam o mecanismo de atenção para capturar interações entre os patches.\n",
    "   - **`mlp_head`**: Camada de normalização (`LayerNorm`) seguida de uma camada linear para gerar a previsão da classe com `num_classes` saídas.\n",
    "   - **`dropout`**: Regularização aplicada tanto no feed-forward quanto nas codificações de entrada.\n",
    "   - **Embeddings**: Usa um **token de classe (`cls_token`)** e **embedding posicional (`pos_embedding`)** para representar informações globais e espaciais.\n",
    "> Obs: fazemos um *positional embedding* simples, que será aprendido. $ PE \\in \\mathbb{R}^{(1 + N) \\times D} $\n",
    "\n",
    "2. **`forward`**:\n",
    "   - **Pré-processamento de Entrada**: A função `img_to_patch` converte a imagem em uma sequência de patches.\n",
    "   - **Token de Classe e Embedding Posicional**: O token de classe é concatenado aos patches, e o embedding posicional é somado para fornecer informações de posição.\n",
    "   - **Aplicação do Transformer**: A sequência de patches passa pelas camadas de Transformer, onde o mecanismo de atenção captura as interações entre eles.\n",
    "   - **Classificação**: O token de classe, após a aplicação do Transformer, é utilizado para a previsão final da classe.\n",
    "\n",
    "Essa estrutura permite que o modelo capture as dependências espaciais das imagens, oferecendo uma alternativa eficiente para tarefas de visão computacional, especialmente quando treinado com grandes conjuntos de dados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a23a62e-67fb-4b74-966b-d02c708f395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_channels, num_heads, num_layers, num_classes, patch_size, num_patches, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1) Defina:\n",
    "        #     - self.patch_size\n",
    "        #     - self.input_layer -> (vetor_do_patch, embed_dim)\n",
    "        #     - self.transformer -> Sequential de TransformerLayer's\n",
    "        #     - self.mlp_head -> mlp simples\n",
    "        #     - self.dropout\n",
    "        #     - self.cls_token\n",
    "        #     - self.pos_embedding -> batch = 1, patches + 1 (cls), embed_dim\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        # 1) Faça \"img_to_patch\"\n",
    "        # 2) Defina algo para x.shape\n",
    "        # 3) Adicione o token CLS (adiciona batch com .repeat) + Positional Encoding\n",
    "        # 4) Faça self.dropout + x.transpose(0, 1)\n",
    "        # 5) Passe para o tranformer\n",
    "        # 6) Classificação:\n",
    "        #     - Pegue o CLS\n",
    "        #     - Classifique-o com MLP\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b686f35f-da08-4707-b28d-3d5f25474d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(trainloader))  # Exemplo de pegar o próximo batch\n",
    "model = VisionTransformer(embed_dim=256, hidden_dim=512, num_channels=3, num_heads=8, num_layers=6, num_classes=args['n_classes'], patch_size=8, num_patches=16, dropout=0.2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98804b3d-e960-4e4d-b626-89ceea684f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(images)\n",
    "print(output.shape)  # Esperado [batch_size, num_classes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3156a96a-8779-42d6-a3a7-a426f5c20763",
   "metadata": {},
   "source": [
    "## Treinamento e Avaliação (Exemplo básico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6a1aae-ab0a-4116-87c8-b07ee52d7a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f814bd8d-9bad-4612-afad-95532c1d98c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir dispositivo\n",
    "device = args['device']\n",
    "model.to(device)\n",
    "\n",
    "# Listas para armazenar as perdas e métricas\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "def train(model, trainloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for images, labels in trainloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = total_correct / total_samples\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    avg_loss = total_loss / len(trainloader)\n",
    "    \n",
    "    print(f\"Perda de treino: {avg_loss:.4f}, Acurácia: {accuracy:.4f}, Precisão: {precision:.4f}, Revocação: {recall:.4f}, F1-score: {f1:.4f}\")\n",
    "    return avg_loss, accuracy, precision, recall, f1\n",
    "\n",
    "def evaluate(model, testloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = total_correct / total_samples\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    avg_loss = total_loss / len(testloader)\n",
    "    \n",
    "    print(f\"Perda de teste: {avg_loss:.4f}, Acurácia: {accuracy:.4f}, Precisão: {precision:.4f}, Revocação: {recall:.4f}, F1-score: {f1:.4f}\")\n",
    "    return avg_loss, accuracy, precision, recall, f1, all_labels, all_preds\n",
    "\n",
    "# Exemplo de treinamento\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nÉpoca {epoch+1}/{num_epochs}\")\n",
    "    train_loss, train_acc, train_prec, train_rec, train_f1 = train(model, trainloader, optimizer, criterion)\n",
    "    test_loss, test_acc, test_prec, test_rec, test_f1, all_labels, all_preds = evaluate(model, testloader, criterion)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "# Plotar o gráfico de perdas\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, num_epochs+1), test_losses, label='Test Loss')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Perda')\n",
    "plt.title('Perda de Treino e Teste')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotar a matriz de confusão\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(conf_matrix, cmap='Blues', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Predito')\n",
    "plt.ylabel('Real')\n",
    "plt.title('Matriz de Confusão')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73a021d-b97e-4dd9-aa23-dad2d4c91f11",
   "metadata": {},
   "source": [
    "## Conclusões\n",
    "\n",
    "Os Vision Transformers (ViTs) são projetados para lidar com tarefas de classificação em larga escala, onde há uma abundância de dados para treinamento. Diferente das CNNs, que exploram a estrutura local das imagens através de convoluções com pesos compartilhados, os ViTs processam a entrada como uma sequência de patches e dependem exclusivamente do mecanismo de atenção para capturar relações espaciais. Essa abordagem, embora poderosa, exige uma quantidade substancial de dados para evitar problemas como overfitting e para permitir que o modelo aprenda representações significativas sem a indutividade estrutural das CNNs.\n",
    "\n",
    "No caso do CIFAR-10, um dataset relativamente pequeno com imagens de 32x32 pixels, torna os ViTs menos eficientes quando treinados do zero. Sem um pré-treinamento em um conjunto de dados maior, o ViT demora mais para convergir e frequentemente apresenta um desempenho inferior em relação às CNNs, que são naturalmente adequadas para esse tipo de tarefa devido à sua capacidade de captar padrões locais desde as primeiras camadas. Como esperado, os resultados preliminares mostraram que o ViT não foi muito bom em eficácia, alcançando uma convergência mais demorada ede menor desempenho com um volume de dados reduzido. No entanto, mesmo nessas condições adversas, o ViT ainda foi capaz de alcançar métricas e perdas boas, com mais épocas, o que demonstra seu potencial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "f5qn",
   "language": "python",
   "name": "f5qn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
