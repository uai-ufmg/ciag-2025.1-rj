{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f64c2354-f15f-450c-bc53-d19ca17ecf99",
   "metadata": {},
   "source": [
    "# ViT Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b808ba95-63cf-4f33-b956-290e04a2ae1c",
   "metadata": {},
   "source": [
    "Como foi explicado no primeiro notebook sobre ViT, os Transformers são modelos que podem ser entendidos como *Encoders*. Isso significa que podemos utilizar o ViT apenas para a extração de *features* do nosso problema e, em seguida, adicionar uma *head* para realizar a tarefa principal, que pode ser classificação, segmentação ou alguma abordagem *multi-modal*. Dessa forma, neste notebook, iremos explorar como usar um ViT pré-treinado, construir uma *head* de segmentação e aplicá-lo à tarefa de segmentação do *Rock Dataset*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafdd8b5-993b-4bb7-ab05-50eff08d40ce",
   "metadata": {},
   "source": [
    "## Configuração\n",
    "\n",
    "Importando módulos necessários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79a5b63-b50b-417d-b1da-54a10f35335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nbproxy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa9c4b0-fd7b-4a06-bc4a-0dd9599a93d2",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06627bf-5a1b-4950-b496-afb322d59255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas principais\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# Transformações e manipulação de dados\n",
    "import torchvision\n",
    "from torchvision import transforms, models, datasets\n",
    "\n",
    "# Utilitários de visualização e manipulação de dados\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from skimage import io, transform\n",
    "from sklearn import metrics\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ViTModel\n",
    "\n",
    "# Divisão de dados\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa35d78-f2cb-4699-9ff9-b497187b773c",
   "metadata": {},
   "source": [
    "## Funções Auxiliares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22de03a6-f784-48d4-a705-ab3679134677",
   "metadata": {},
   "source": [
    "Funções para avaliação de métricas, apresentação de imagens e das predições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5237dc9-3b5f-4d69-ae7a-7fdbf76ab4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(preds, labels):\n",
    "    f1_list = []\n",
    "    iou_list = []\n",
    "\n",
    "    for i in tqdm(range(len(preds)), desc='Metrics'):\n",
    "        f1 = metrics.f1_score(labels[i].flatten(), preds[i].flatten())\n",
    "        iou = metrics.jaccard_score(labels[i].flatten(), preds[i].flatten())\n",
    "\n",
    "        f1_list.append(f1)\n",
    "        iou_list.append(iou)\n",
    "\n",
    "    f1_list = np.asarray(f1_list)\n",
    "    iou_list = np.asarray(iou_list)\n",
    "\n",
    "    return f1_list, iou_list\n",
    "\n",
    "def show_image(img, mask):\n",
    "    img = img.cpu().numpy().transpose(1, 2, 0)  # (C, H, W) -> (H, W, C)\n",
    "    img = np.clip(img, 0, 1) \n",
    "    \n",
    "    if mask.ndim == 3:\n",
    "        mask = mask.squeeze(0)  # (1, H, W) -> (H, W)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].axis('off') \n",
    "    axes[0].set_title(\"Imagem\")\n",
    "\n",
    "    axes[1].imshow(mask, cmap='gray')\n",
    "    axes[1].axis('off')  \n",
    "    axes[1].set_title(\"Máscara\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def show_image_and_mask_prediction(img, mask, pred):\n",
    "    img = img.cpu().numpy().transpose(1, 2, 0)  # (C, H, W) -> (H, W, C)\n",
    "    img = np.clip(img, 0, 1)  \n",
    "\n",
    "    if mask.ndim == 3:  \n",
    "        mask = mask.squeeze(0)  # (1, H, W) -> (H, W)\n",
    "        \n",
    "    pred = torch.argmax(pred, dim=0).cpu().numpy() \n",
    "    \n",
    "    if pred.ndim == 3: \n",
    "        pred = pred.squeeze(0)  # (1, H, W) -> (H, W)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].axis('off')  \n",
    "    axes[0].set_title(\"Imagem\")\n",
    "\n",
    "    axes[1].imshow(mask, cmap='gray')\n",
    "    axes[1].axis('off')  \n",
    "    axes[1].set_title(\"Máscara\")\n",
    "\n",
    "    axes[2].imshow(pred, cmap='gray')\n",
    "    axes[2].axis('off')  \n",
    "    axes[2].set_title(\"Previsão\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f023697-322a-470d-8e4a-f8034cf8ec64",
   "metadata": {},
   "source": [
    "## Dataset - RockDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d329a9e5-f260-4e28-8800-83893b208c6e",
   "metadata": {},
   "source": [
    "## Carregamento do Dataset `Rockdataset`\n",
    "\n",
    "Aqui será feito o carregamento do dataset `Rockdataset`, que já foi utilizado em uma tarefa anterior de segmentação com uma rede convolucional U-Net, na semana de CNN. \n",
    "\n",
    "O carregamento de dados será diferente desta vez, pois utilizaremos uma versão pré-pronta e pré-treinada do Vision Transformer para essa tarefa. Essa abordagem exige que a imagem e a máscara estejam em um formato específico:\n",
    "\n",
    "- Imagem: `(batch_size, 3 , altura, largura)` → 3 canais (RGB), 224x224\n",
    "- Máscara: `(batch_size, 1, altura, largura)` → 1 canal (grayscal, 224x224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294423e4-28d7-457c-981c-84af67320f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RockDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            is_train: bool,\n",
    "            crop_size: int = 256,\n",
    "            num_classes: int = 2,\n",
    "            datapath: str = '/pgeoprj2/ciag2024/dados/drp-benchmarks/images/grosmont',\n",
    "            transform = None\n",
    "        ):\n",
    "        self.is_train = is_train\n",
    "        self.crop_size = crop_size\n",
    "        self.num_classes = num_classes\n",
    " \n",
    "        self.img_path = os.path.join(datapath, 'tif')\n",
    "        self.mask_path = os.path.join(datapath, 'segmented-kongju.raw')\n",
    " \n",
    "        self.image_filenames, self.mask_vol = self.make_dataset()\n",
    " \n",
    "        self.transform = transform\n",
    " \n",
    "        if len(self.image_filenames) == 0:\n",
    "            raise RuntimeError('Found 0 images, please check the data set.')\n",
    " \n",
    "    def make_dataset(self):\n",
    "        if self.is_train:\n",
    "            mask_vol = np.fromfile(open(self.mask_path, 'rb'), dtype=np.int8).reshape(1024, 1024, 1024)\n",
    "            mask_vol = mask_vol.transpose((0, 2, 1))[:-64]\n",
    "        else:\n",
    "            mask_vol = np.fromfile(open(self.mask_path, 'rb'), dtype=np.int8).reshape(1024, 1024, 1024)\n",
    "            mask_vol = mask_vol.transpose((0, 2, 1))[-64:]\n",
    " \n",
    "        if self.is_train:\n",
    "            image_filenames = sorted([f for f in os.listdir(self.img_path) if os.path.isfile(os.path.join(self.img_path, f))])[:-64]\n",
    "        else:\n",
    "            image_filenames = sorted([f for f in os.listdir(self.img_path) if os.path.isfile(os.path.join(self.img_path, f))])[-64:]\n",
    " \n",
    "        return image_filenames, mask_vol\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.image_filenames[idx]\n",
    "        img_path = os.path.join(self.img_path, filename)\n",
    "    \n",
    "        img = io.imread(img_path)\n",
    "        mask = self.mask_vol[idx]\n",
    "    \n",
    "        # Fazendo o casting apropriado\n",
    "        img = img.astype(np.float32)\n",
    "        mask = mask.astype(np.int64)\n",
    "    \n",
    "        # Z-Score Normalization\n",
    "        img = (img - img.mean()) / img.std()\n",
    "    \n",
    "        # Se for treino: crop aleatório\n",
    "        if self.is_train:\n",
    "            randh, randw = np.random.randint(0, 1024 - self.crop_size, size=2)\n",
    "            img = img[randh:randh+self.crop_size, randw:randw+self.crop_size]\n",
    "            mask = mask[randh:randh+self.crop_size, randw:randw+self.crop_size]\n",
    "            \n",
    "            img = np.stack([img, img, img], axis=0)  # (3, 256, 256)\n",
    "            mask = np.expand_dims(mask, axis=0)  # (1, 256, 256)\n",
    "        \n",
    "        else:  # Validação: dividir em 16 partes e retornar cada uma separadamente\n",
    "            patches = [\n",
    "                (img[i:i+256, j:j+256], mask[i:i+256, j:j+256])\n",
    "                for i in range(0, 1024, 256) for j in range(0, 1024, 256)\n",
    "            ]\n",
    "        \n",
    "            img, mask = patches[idx % 16]  # Cada chamada retorna um patch individual\n",
    "            \n",
    "            img = np.stack([img, img, img], axis=0)  # (3, 256, 256)\n",
    "            mask = np.expand_dims(mask, axis=0)  # (1, 256, 256)\n",
    "    \n",
    "        # Convertendo para tensor\n",
    "        img = torch.tensor(img, dtype=torch.float32)\n",
    "        mask = torch.tensor(mask, dtype=torch.int64)\n",
    "    \n",
    "        if self.transform:\n",
    "            img = self.transform(img)    # (3, 224, 224)\n",
    "            mask = self.transform(mask)  # (1, 224, 224)\n",
    "    \n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba5b6a4-b517-44d9-9150-bd9e347e9617",
   "metadata": {},
   "source": [
    "### Datasets e Dataloaders\n",
    "#### 1) Crie os Dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b84076d-8b1f-4c83-8b95-d20635cd22a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transformação com Compose\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Redimensiona para o tamanho desejado\n",
    "])\n",
    "\n",
    "# 1) train_dataset =\n",
    "# 2) valid_dataset =\n",
    "\n",
    "print('Número de instâncias de treino:', len(train_dataset))\n",
    "print('Número de instâncias de validação:', len(valid_dataset))\n",
    "\n",
    "\n",
    "# 1) train_dataloader = -> batch_size=32, shuffle=True, num_workers=4\n",
    "# 2) valid_dataloader = -> batch_size=32, shuffle=False, num_workers=4\n",
    "\n",
    "\n",
    "\n",
    "# Verificando tamanhos e dados esperados\n",
    "for imgs, masks in train_dataloader:\n",
    "    print(imgs.shape)  # Esperado: torch.Size([32, 3, 224, 224])\n",
    "    print(masks.shape)  # Esperado: torch.Size([32, 1, 224, 224])\n",
    "    print(imgs.dtype, masks.dtype) # Esperado: torch.float32 torch.int64\n",
    "    break\n",
    "# Exemplo de iteração para validação\n",
    "for imgs, masks in val_dataloader:\n",
    "    print(imgs.shape)  # Esperado: torch.Size([32, 3, 224, 224])\n",
    "    print(masks.shape)  # Esperado: torch.Size([32, 1, 224, 224])\n",
    "    print(imgs.dtype, masks.dtype) # Esperado: torch.float32 torch.int64\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01413d2a-567f-4fad-9cd5-4c1e77d88f94",
   "metadata": {},
   "source": [
    "### Exemplo de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c15ef3-4c7c-4e25-b21e-acb06466ce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for imgs, masks in train_dataloader:\n",
    "    print(\"Exibindo imagens do batch:\")\n",
    "    \n",
    "    for i in range(1):\n",
    "        show_image(imgs[i], masks[i])  # Mostrando a imagem i do batch\n",
    "    break  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b95f14-486e-49c2-b97e-dbc643588c39",
   "metadata": {},
   "source": [
    "## Definindo o Modelo - ViT Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1c6b81-c5d7-4eec-b1fb-bb5fa23ddff0",
   "metadata": {},
   "source": [
    "Aqui iremos definir a classe do nosso modelo `ViTSegmentation`. Para isso, utilizaremos um ViT pré-treinado (`vit_model`) como *encoder* e, em seguida, aplicaremos um *decoder* (`head`), que consistirá em uma camada `Conv2D` simples para realizar a segmentação. Após essa etapa, faremos um *upsample* para que a saída do modelo tenha o mesmo tamanho da máscara (`mask`).\n",
    "#### 2) Defina ViTSegmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14db37c0-1f21-4436-a532-89f7f00d56d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTSegmentation(nn.Module):\n",
    "    def __init__(self, vit_model, num_classes):\n",
    "        super(ViTSegmentation, self).__init__()\n",
    "\n",
    "        # 1) Defina self.vit\n",
    "        # 2) Defina self.decoder -> 768 é a dimensão do embedding do ViT\n",
    "        # 3) defina self.upsample -> fator 16x\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # 1) Passe o input pelo ViT\n",
    "        # 2) Pegue o último estado com .last_hidden_state -> Formato: (batch_size, num_patches + 1, 768)\n",
    "        # 3) Tire o CLS (não iremos classificar, mas ele vem de padrão no modelo pré-trinado)\n",
    "        # 4) Defina algo para patches.shape\n",
    "        # 5) Defina patch_size = sqrt(num_patches)\n",
    "        # 6) Redimensionar para (batch_size, embedding_dim, patch_size, patch_size)\n",
    "        # 7) Passe pelo Decoder\n",
    "        # 8) Faça o Upsample\n",
    "\n",
    "  \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4059a533-273d-4940-a105-0bb2aa6e1123",
   "metadata": {},
   "source": [
    "#### Carregando o modelo pré-treinado\n",
    "> É normal que dê algum Warning de peso não inicializado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a9eefe-50c6-4b63-ae6f-b4a7d0fa1bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar ViT pré-treinado\n",
    "vit_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "model = ViTSegmentation(vit_model, num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd98abb5-3b5d-44e6-940a-66fddaaf3843",
   "metadata": {},
   "source": [
    "### Testando plots e tamanhos passados no modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e1bc39-02a3-4e7a-b38e-b7c8ba047a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, masks in val_dataloader:\n",
    "    print(f\"Batch de imagens: {images.shape}\")\n",
    "    print(f\"Batch de máscaras: {masks.shape}\")\n",
    "    outputs = model(images)\n",
    "    print(f\"Saída do modelo: {outputs.shape}\")\n",
    "\n",
    "    show_image_and_mask_prediction(images[31], masks[31], outputs[31])\n",
    "    break\n",
    "\n",
    "# Resultado Esperado\n",
    "# Batch de imagens: torch.Size([32, 3, 224, 224])\n",
    "# Batch de máscaras: torch.Size([32, 1, 224, 224])\n",
    "# Saída do modelo: torch.Size([32, 2, 224, 224])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db837be1-1112-4b9d-9342-693a2af68da5",
   "metadata": {},
   "source": [
    "## Treinamento e Avaliação (Exemplo básico)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969b25c8-a7e0-4e6f-b1ed-1c4668f171e9",
   "metadata": {},
   "source": [
    "#### 3) Escolha o otimizador e loss mais adequados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57749ceb-3900-45fe-847a-fa8c88ad3924",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 5e-4\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Escolha o otimizador e loss mais adequados\n",
    "# 1) optimizer =\n",
    "# 2) criterion =\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73113d35-f27e-43e7-9606-e3fd6199fcc6",
   "metadata": {},
   "source": [
    "#### 4) Faça o loop de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ad40ea-4fed-4af6-93d5-8e8c1a5f57b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, criterion, optimizer, device):\n",
    "    tic = time.time()\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    all_labels, all_preds = [], []\n",
    "    \n",
    "    for i, batch in (pbar := tqdm(enumerate(train_dataloader), total=len(train_dataloader), unit='batch')):\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        labels = labels.squeeze(1).long()\n",
    "\n",
    "        # Faça o loop de treino padrão\n",
    "        # .\n",
    "        # .\n",
    "        # .\n",
    "        \n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        pbar.set_description(f\"Train loss: {np.mean(train_losses):.4f}\")\n",
    "    \n",
    "    f1, iou = evaluate(all_preds, all_labels)\n",
    "    tac = time.time()\n",
    "    \n",
    "    print('[train], [loss %.4f +/- %.4f], [iou %.4f +/- %.4f], [f1 %.4f +/- %.4f], [time %.2f]' % (\n",
    "        np.mean(train_losses), np.std(train_losses), iou.mean(), iou.std(), f1.mean(), f1.std(), (tac - tic)))\n",
    "\n",
    "def validate(model, valid_dataloader, criterion, epoch, device):\n",
    "    tic = time.time()\n",
    "    display_images_freq = 2\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    valid_losses = []\n",
    "    all_labels, all_preds = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in (pbar := tqdm(enumerate(valid_dataloader), total=len(valid_dataloader), unit='batch')):\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            labels = labels.squeeze(1).long()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            \n",
    "            valid_losses.append(loss.item())\n",
    "            pbar.set_description(f\"Valid loss: {np.mean(valid_losses):.4f}\")\n",
    "            \n",
    "            if i == 0 and epoch % display_images_freq == 0:\n",
    "                fig, ax = plt.subplots(2, 3, figsize=(10, 7))\n",
    "                perm = np.random.permutation(inputs.size(0))\n",
    "                \n",
    "                for p in range(2):\n",
    "                    ax[p, 0].imshow(inputs[perm[p], 0].cpu().numpy())\n",
    "                    ax[p, 0].set_title('Image')\n",
    "                    ax[p, 1].imshow(labels[perm[p]].cpu().numpy())\n",
    "                    ax[p, 1].set_title('Label')\n",
    "                    ax[p, 2].imshow(preds[perm[p]].cpu().numpy())\n",
    "                    ax[p, 2].set_title('Prediction')\n",
    "                    \n",
    "                    for j in range(3):\n",
    "                        ax[p, j].set_xticks([])\n",
    "                        ax[p, j].set_yticks([])\n",
    "                \n",
    "                plt.show()\n",
    "    \n",
    "    f1, iou = evaluate(all_preds, all_labels)\n",
    "    tac = time.time()\n",
    "    \n",
    "    print('[test], [loss %.4f +/- %.4f], [iou %.4f +/- %.4f], [f1 %.4f +/- %.4f], [time %.2f]' % (\n",
    "        np.mean(valid_losses), np.std(valid_losses), iou.mean(), iou.std(), f1.mean(), f1.std(), (tac - tic)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e02f48-d8fb-4574-868c-4b95a6e2c20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(f' ========== Epoch {epoch} ========== ')\n",
    "\n",
    "    train(model, train_dataloader, criterion, optimizer, device)\n",
    "    validate(model, val_dataloader, criterion, epoch, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a860924-6099-45bb-93b5-40fc7a4980b9",
   "metadata": {},
   "source": [
    "## Conclusões"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9a34cc-0445-4b82-8796-33ded2700584",
   "metadata": {},
   "source": [
    "Assim como no primeiro notebook sobre ViT, essa mesma questão persiste: esse tipo de arquitetura precisa de muitos dados e tempo de treinamento para obter um bom desempenho nas tarefas que lhe forem atribuídas.\n",
    "\n",
    "Dessa forma, o modelo levará muito mais tempo para convergir. Mesmo utilizando um modelo pré-treinado, ele foi ajustado para problemas de classificação, o que dificulta a adaptação para uma nova tarefa. Além disso, nossa cabeça de segmentação é extremamente simples, consistindo apenas em uma única convolução. Um cabeçote mais elaborado poderia melhorar os resultados das previsões.\n",
    "\n",
    "Ainda assim, conseguimos métricas razoáveis e previsões coerentes, embora inferiores às obtidas com a UNet padrão, que alcançou bons resultados de forma muito mais rápida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d409dad-c821-41ca-9135-5869f5371a48",
   "metadata": {},
   "source": [
    "## **Desafio**\n",
    "> Faça o ViT *from scratch*, defina TransformerLayer, TransformerEncoder, e uma SegmentationHead, gerando um modelo completo de segmentação que seria treinado do 0! Veja como ficam os resultados! (É normal que ele não fique bom, já que são modelos que demoram muito para treinar e *\"fine-tunar\"*).\n",
    "\n",
    "> Se quiser, também pode buscar outros modelos, pré-treinados, ou usar o mesmo ViT model que fizemos acima e só usar uma SegmentationHead melhor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d370693d-b1ae-40f7-a1ae-69838f88b910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sua entrada tem o seguinte formato torch.Size([32, 3, 224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c529b258-08a2-4a7d-b8f9-af5e4d9b6b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_patch(x, patch_size):\n",
    "    B, C, H, W = x.shape\n",
    "    x = x.reshape(B, C, H // patch_size, patch_size, W // patch_size, patch_size)\n",
    "    x = x.permute(0, 2, 4, 1, 3, 5)\n",
    "    x = x.flatten(1,2)\n",
    "    x = x.flatten(2,4)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f47cb16-674e-4053-bc41-979793bf501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, label in train_dataloader: \n",
    "    print(img.shape, label.shape)  # Esperado = torch.Size([32, 3, 224, 224]) torch.Size([32, 1, 224, 224])\n",
    "    x = img_to_patch(img, 16)      \n",
    "    print(x.shape)                 # Esperado = torch.Size([32, 196, 768])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ad0627-a0ea-42bc-8870-d3699a2006d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Layer\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dd420b-aceb-403b-a353-9771bca1fcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(32, 196, 768)\n",
    "\n",
    "transformer_layer = TransformerLayer(embed_dim=768, hidden_dim=384, num_heads=8, dropout=0.1)\n",
    "output = transformer_layer(x)\n",
    "\n",
    "print(output.shape) # Esperado: torch.Size([32, 196, 768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c359a949-9bf0-408a-a2d3-326c2838be48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationHead(nn.Module):\n",
    "    def __init__(self, in_channels=768, num_classes=1):\n",
    "        super(SegmentationHead, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e1cec7-6094-435a-9918-6c677bf139f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(32, 768, 14, 14)\n",
    "\n",
    "segmentation_head = SegmentationHead()\n",
    "output = segmentation_head(x)\n",
    "\n",
    "print(output.shape)  # Esperado = torch.Size([32, 1, 224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a141e3-0a3f-4fdb-90b4-64e218bb43a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vit_Seg(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_channels, num_heads, num_layers, num_classes, patch_size, num_patches, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        # ...\n",
    "        self.transformer = ...\n",
    "        self.segmentation_head = ...\n",
    "        # ...     \n",
    "       \n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # ...\n",
    "        x = self.transformer(x)\n",
    "        B, N, C = x.shape\n",
    "        # ...\n",
    "        x = self.segmentation_head(x) # Espera uma entrada de tamanhos [(32, 768, 14, 14)]\n",
    "        # ...\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543f9a5b-9e74-4340-a3a6-e8f98c742984",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(32, 3, 224, 224)\n",
    "\n",
    "model = Vit_Seg(embed_dim=768, hidden_dim=384, num_channels=3, num_heads=8, num_layers=4, num_classes=2, patch_size=16, num_patches=196, dropout=0.1)\n",
    "output = model(x)\n",
    "\n",
    "print(output.shape)  # Esperado = torch.Size([32, 1, 224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51b7e0f-cd76-42c6-8b6d-aadac6ad8949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, criterion, optimizer, device):\n",
    "    tic = time.time()\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    all_labels, all_preds = [], []\n",
    "    \n",
    "    for i, batch in (pbar := tqdm(enumerate(train_dataloader), total=len(train_dataloader), unit='batch')):\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # labels = labels.squeeze(1).long() -> p/ CrossEntropy\n",
    "        labels = labels.float() # -> p/ BCELoss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        pbar.set_description(f\"Train loss: {np.mean(train_losses):.4f}\")\n",
    "\n",
    "def validate(model, valid_dataloader, criterion, epoch, device):\n",
    "    tic = time.time()\n",
    "    display_metrics_freq = 5\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    valid_losses = []\n",
    "    all_labels, all_preds = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in (pbar := tqdm(enumerate(valid_dataloader), total=len(valid_dataloader), unit='batch')):\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # labels = labels.squeeze(1).long() -> p/ CrossEntropy\n",
    "            labels = labels.float() # -> p/ BCELoss\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            \n",
    "            valid_losses.append(loss.item())\n",
    "            pbar.set_description(f\"Valid loss: {np.mean(valid_losses):.4f}\")\n",
    "            \n",
    "            if i == 0 and epoch % display_metrics_freq == 0:\n",
    "                f1, iou = evaluate(all_preds, all_labels)\n",
    "                tac = time.time()\n",
    "    \n",
    "                print('[test], [loss %.4f +/- %.4f], [iou %.4f +/- %.4f], [f1 %.4f +/- %.4f], [time %.2f]' % (\n",
    "                    np.mean(valid_losses), np.std(valid_losses), iou.mean(), iou.std(), f1.mean(), f1.std(), (tac - tic)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3c6e3b-c366-43af-98b3-95c2c930d420",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 5e-4\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Escolha o otimizador e loss mais adequados -> Dica BCELoss ou CrossEntropy\n",
    "# 1) optimizer =\n",
    "# 2) criterion ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dd3440-3280-4588-b230-f14d0c379fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(f' ========== Epoch {epoch} ========== ')\n",
    "\n",
    "    train(model, train_dataloader, criterion, optimizer, device)\n",
    "    validate(model, val_dataloader, criterion, epoch, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bb2229-f8ab-4866-b757-c587423e7a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste mais coisas !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df83d48d-52a2-4023-a0a2-f0b30a65982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7999790-dc26-4f0f-a526-8d85988f787f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "f5qn",
   "language": "python",
   "name": "f5qn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
